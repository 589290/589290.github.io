{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"q7.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood ###\n",
    "\n",
    "Probability will be one of our best friends as we go through Deep Learning. In this lesson, we'll see how we can use probability to evaluate (and improve!) our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div align=\"middle\"><video width=\"100%\" controls><source src=\"v12.mp4\" type=\"video/mp4\"></video></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"<div align=\"middle\"><video width=\"100%\" controls><source src=\"v12.mp4\" type=\"video/mp4\"></video></div>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximizing Probabilities ###\n",
    "In this lesson and quiz, we will learn how to maximize a probability, using some math. Nothing more than high school math, so get ready for a trip down memory lane!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log(ab) = log(a) + log(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div align=\"middle\"><video width=\"100%\" controls><source src=\"v13.mp4\" type=\"video/mp4\"></video></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"<div align=\"middle\"><video width=\"100%\" controls><source src=\"v13.mp4\" type=\"video/mp4\"></video></div>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div align=\"middle\"><video width=\"100%\" controls><source src=\"v14.mp4\" type=\"video/mp4\"></video></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"<div align=\"middle\"><video width=\"100%\" controls><source src=\"v14.mp4\" type=\"video/mp4\"></video></div>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div align=\"middle\"><video width=\"100%\" controls><source src=\"v15.mp4\" type=\"video/mp4\"></video></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"<div align=\"middle\"><video width=\"100%\" controls><source src=\"v15.mp4\" type=\"video/mp4\"></video></div>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div align=\"middle\"><video width=\"100%\" controls><source src=\"v16.mp4\" type=\"video/mp4\"></video></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"<div align=\"middle\"><video width=\"100%\" controls><source src=\"v16.mp4\" type=\"video/mp4\"></video></div>\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Write a function that takes as input two lists Y, P,\n",
    "# and returns the float corresponding to their cross-entropy.\n",
    "\n",
    "def cross_entropy(Y, P):\n",
    "    Y = np.float_(Y)\n",
    "    P = np.float_(P)\n",
    "    return -np.sum(Y * np.log(P) + (1 - Y) * np.log(1 - P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div align=\"middle\"><video width=\"100%\" controls><source src=\"v17.mp4\" type=\"video/mp4\"></video></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"<div align=\"middle\"><video width=\"100%\" controls><source src=\"v17.mp4\" type=\"video/mp4\"></video></div>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression ###\n",
    "\n",
    "Now, we're finally ready for one of the most popular and useful algorithms in Machine Learning, and the building block of all that constitutes Deep Learning. The Logistic Regression Algorithm. And it basically goes like this:\n",
    "\n",
    "Take your data\n",
    "Pick a random model\n",
    "Calculate the error\n",
    "Minimize the error, and obtain a better model\n",
    "Enjoy!\n",
    "\n",
    "### Calculating the Error Function ###\n",
    "\n",
    "Let's dive into the details. The next video will show you how to calculate an error function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div align=\"middle\"><video width=\"100%\" controls><source src=\"v18.mp4\" type=\"video/mp4\"></video></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"<div align=\"middle\"><video width=\"100%\" controls><source src=\"v18.mp4\" type=\"video/mp4\"></video></div>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div align=\"middle\"><video width=\"100%\" controls><source src=\"v19.mp4\" type=\"video/mp4\"></video></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"<div align=\"middle\"><video width=\"100%\" controls><source src=\"v19.mp4\" type=\"video/mp4\"></video></div>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div align=\"middle\"><video width=\"100%\" controls><source src=\"v20.mp4\" type=\"video/mp4\"></video></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"<div align=\"middle\"><video width=\"100%\" controls><source src=\"v20.mp4\" type=\"video/mp4\"></video></div>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Calculation ###\n",
    "\n",
    "In the last few videos, we learned that in order to minimize the error function, we need to take some derivatives. So let's get our hands dirty and actually compute the derivative of the error function. The first thing to notice is that the sigmoid function has a really nice derivative. Namely,\n",
    "\n",
    "$\\sigma'(x) = \\sigma(x) (1-\\sigma(x))Ïƒ$\n",
    "\n",
    "The reason for this is the following, we can calculate it using the quotient formula:\n",
    "\n",
    "<img src=\"q8.gif\" width=300>\n",
    "\n",
    "And now, let's recall that if we have mm points labelled $x^{(1)}, x^{(2)}, \\ldots, x^{(m)}$, the error formula is:\n",
    "\n",
    "$E = -\\frac{1}{m} \\sum_{i=1}^m \\left( y_i \\ln(\\hat{y_i}) + (1-y_i) \\ln (1-\\hat{y_i}) \\right)$\n",
    "\n",
    "where the prediction is given by $\\hat{y_i} = \\sigma(Wx^{(i)} + b)$.\n",
    "\n",
    "Our goal is to calculate the gradient of $E$, at a point $x = (x_1, \\ldots, x_n)$, given by the partial derivatives\n",
    "\n",
    "$\\nabla E =\\left(\\frac{\\partial}{\\partial w_1}E, \\cdots, \\frac{\\partial}{\\partial w_n}E, \\frac{\\partial}{\\partial b}E \\right)$\n",
    "\n",
    "To simplify our calculations, we'll actually think of the error that each point produces, and calculate the derivative of this error. The total error, then, is the average of the errors at all the points. The error produced by each point is, simply,\n",
    "\n",
    "$E = - y \\ln(\\hat{y}) - (1-y) \\ln (1-\\hat{y})$\n",
    "\n",
    "In order to calculate the derivative of this error with respect to the weights, we'll first calculate $\\frac{\\partial}{\\partial w_j} \\hat{y}$. \n",
    "Recall that $\\hat{y} = \\sigma(Wx+b)$\n",
    "\n",
    "<img src=\"q9.gif\" width=500>\n",
    "\n",
    "The last equality is because the only term in the sum which is not a constant with respect to $w_j$ is precisely $w_j x_j$, which clearly has derivative $x_j$.\n",
    "\n",
    "Now, we can go ahead and calculate the derivative of the error $E$ at a point $x$, with respect to the weight $w_j$.\n",
    "\n",
    "<img src=\"qa.png\" width=500>\n",
    "\n",
    "A similar calculation will show us that\n",
    "\n",
    "<img src=\"qb.gif\" width=170>\n",
    "\n",
    "This actually tells us something very important. For a point with coordinates $(x_1, \\ldots, x_n)$, label y,y, and prediction $\\hat{y}$, the gradient of the error function at that point is $\\left(-(y - \\hat{y})x_1, \\cdots, -(y - \\hat{y})x_n, -(y - \\hat{y}) \\right)$. In summary, the gradient is\n",
    "\n",
    "$\\nabla E = -(y - \\hat{y}) (x_1, \\ldots, x_n, 1)$.\n",
    "\n",
    "If you think about it, this is fascinating. The gradient is actually a scalar times the coordinates of the point! And what is the scalar? Nothing less than a multiple of the difference between the label and the prediction. What significance does this have?\n",
    "\n",
    "__Closer the label to the prediction, smaller the gradient.__\n",
    "\n",
    "__Farther the label from the prediction, larger the gradient.__\n",
    "\n",
    "So, a small gradient means we'll change our coordinates by a little bit, and a large gradient means we'll change our coordinates by a lot.\n",
    "\n",
    "If this sounds anything like the perceptron algorithm, this is no coincidence! We'll see it in a bit.\n",
    "\n",
    "### Gradient Descent Step ###\n",
    "\n",
    "Therefore, since the gradient descent step simply consists in subtracting a multiple of the gradient of the error function at every point, then this updates the weights in the following way:\n",
    "\n",
    "$w_i' \\leftarrow w_i -\\alpha [-(y - \\hat{y}) x_i]$,\n",
    "\n",
    "which is equivalent to\n",
    "\n",
    "$w_i' \\leftarrow w_i + \\alpha (y - \\hat{y}) x_i$.\n",
    "\n",
    "Similarly, it updates the bias in the following way:\n",
    "\n",
    "$b' \\leftarrow b + \\alpha (y - \\hat{y}),$\n",
    "\n",
    "Note: Since we've taken the average of the errors, the term we are adding should be $\\frac{1}{m} \\cdot \\alpha$ instead of $\\alpha$, but as $\\alpha$ is a constant, then in order to simplify calculations, we'll just take $\\frac{1}{m} \\cdot \\alpha$ to be our learning rate, and abuse the notation by just calling it $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div align=\"middle\"><video width=\"100%\" controls><source src=\"v21.mp4\" type=\"video/mp4\"></video></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"<div align=\"middle\"><video width=\"100%\" controls><source src=\"v21.mp4\" type=\"video/mp4\"></video></div>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
